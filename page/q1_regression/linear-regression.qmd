---
title: "重回帰分析"
permalink: /qreg_linear-regression/
toc_label: "目次"
header.image: "assets/header/linear-regression.jpg"
---


この記事では、定量的な分析の基本的な手法である回帰分析のうち、もっとも基本的な線形重回帰分析について扱います。なお、他の回帰分析にも通じる根本的な概念を説明しているため、かなり長くなっています。他の手法でわからない説明も、ここに戻ってくれば見つかるかもしれません。

## 重要な用語と、少しだけ数学
### 独立変数、従属変数

多くの実証的な研究の仮説は、原因と結果の間に存在する因果メカニズムまたはその効果に関する記述です。この枠組みにおいて、原因の部分を独立変数といい、結果の部分を従属変数といいます。統計学または計量経済学での数式の慣習から、一般的に独立変数は $X$ で、従属変数は $Y$ で表されます。また、独立変数が複数ある場合は $X_1, X_2, \cdots$ というふうに、右下に数字をつけることにします。

独立変数はほかにも、説明変数、原因変数、処置変数などと呼ぶことがあります。従属変数は、非説明変数、結果変数などと呼ぶことがあります。これらは主に分野による慣習の違いですが、どれを聞いてもどちらかわかるようにしておいてください。なお英語では、それぞれ Independent Variable と Dependent Variable です。

### 確率変数

確率変数とは、その値が確率的に決まる変数のことをいいます。サイコロの出目などがわかり易い例です。計量政治学では、多くの変数は確率的に決まるものとみなされます。例えばある国の民主主義指数とか、戦争を起こしたかどうかとか、選挙の結果とか。テストの得点も確率変数です。これは、「あなたが95点を確率的に（たまたま）取った」という意味ではなく、「95点を含むあらゆる得点は事前に決まっている確率的な分布に従って現れた 」という意味です。ちなみにテストの得点はたいてい、正規分布に従うような分布をしています。不思議ですね。

### 期待値、条件付き期待値

期待値とは、ある確率変数が確率的にとる値の平均のことをいいます。確率変数 $X$ の期待値は $E[X]$ で表されます。例えば、 $X$  をサイコロの出目とすれば、 $E[X] = 3.5$ となります (定義や計算は省略)。もちろん、サイコロを振っても3.5という出目はでませんし、3.5 に近い値が出やすいというわけでもありません。あくまで、平均をとると3.5になるという意味です。

条件付き期待値とは、ある事象が与えられたという条件のもとでの、ある確率変数が確率的に取る値の平均をいいます。別の確率変数 $Z$ が与えられたときの変数 $X$  の条件付き期待値は、 $E[X \| Z]$ と表し、これは変数 $Z$ の値によって変わるので、 $Z$ の関数です。とくに、 $Z = z$ のときの変数 $X$  の条件付き期待値を $E[X \| Z = z]$ と表します。例えば、全校生徒のテストの点数を $X$ とすれば、 $E[X \| grade = 3]$  は3年生の得点の期待値となります。 

このシリーズを読むのに必要な数学の知識はこれくらいです。ここまでの数学知識をきちんと学びたい方は、統計学 (確率論ではない) のテキストを読んでください。

## 回帰モデルの基礎

いま、 $Y$ に与える影響を知りたい変数が $M$ 個あるとし、それらを $X_1, \cdots, X_M$ とします。ただし、いちいち書き並べるのは面倒なので、太字で $\boldsymbol{X} = (X_1, \cdots, X_M)$ と書くことにしましょう。そして、独立変数と従属変数の間に次の一次関数の関係があると仮定します[]。

[]: この「仮定」という考えが、実はとても重要だと考えています。すなわち、実際にそうであるかは未知 (神のみぞ知る) なのだが、一旦そのような関係性を仮定し、そこから政治学的に重要な示唆を数学的に導き出すという手順が、「モデル」に基づく推測の本質です。

$$
Y = \beta_0 + \beta_1X_1 + \cdots + \beta_M X_M + \varepsilon
$$

この式を回帰式とか回帰モデルといいます。この式は、中学生のときに学んだ一次関数の式を拡張したものです。右辺の $\beta_0$ は切片を表し、 $\beta_1$ は変数 $X_1$ の係数、つまり傾きを表しています。中学校の教科書によれば、一次関数の傾きは（ $y$ の増加量）÷ ( $x$ の増加量）で求められるのでした。つまり係数とは、 $x$ が「1単位」増加したときに $y$ がどの程度増加するかを表しています。

このことを今紹介したモデルに当てはめれば、ある変数 $X_k$ が1増加したとき、 $Y$ の期待値は $\beta_k$ だけ増加することになります。

この式から、ある示唆が得られます。すなわち、 $\beta_0, \beta_1, \cdots, \beta_M$ の値がわかれば、各独立変数 $X$ が従属変数 $Y$  に与える影響の大きさがわかるということです[]。したがって、 $\beta_0, \beta_1, \cdots, \beta_M$ の値を知ること (推定すること) が、回帰分析の次なる目標になります。なおこれ以降、やはり書き並べるのが面倒なので、太字で $\boldsymbol{\beta} = (\beta_1, \cdots, \beta_M)$ と書くことにします。

[]: 厳密には行間があります。二つの変数の間には、片方が変化すればもう一方が変化するという相関関係が仮定されており、係数の値はその変化の割合を表しています。しかし、この関係が因果関係であるとは限らず、係数が因果効果であるためには理論的・統計的にさらに強い仮定が必要です。

M個の独立変数 $\boldsymbol{X}$ によってでも、$Y$ の値は完全に説明されるとは限りません。他にさらなる変数が存在したり、測定による誤差が生じたりするかもしれないためです。このような、独立変数 $\boldsymbol{X}$ で予測される $Y$ の値と観測された真の値との差を表す部分を誤差項といい、式の中の $\varepsilon$ がこれに当たります。ここではひとまず、独立変数 $\boldsymbol{X}$ 以外によって説明される部分というふうに理解してください。

### $\boldsymbol{\beta}$ の推定

回帰式を立てたとて、最も重要な回帰係数 $\boldsymbol{\beta}$ の値は私たちには知りようがありません。それは独立変数と従属変数の真の関係性であって、それを決めた神のような存在しか知り得ないためです。しかし人類は、これらの値を観察されたデータから推定する方法をいくつか発明しました。その中で最も有名な方法の一つが最小二乗法（OLS: Ordinary Least Squares）です。最小二乗法の具体的な計算方法はこのブログのレベルを超えるため述べません。一方、最小二乗法で計算した $\boldsymbol{\beta}$ の推定値が妥当である[]ためには、いくつかの仮定をクリアする必要があります。これらの仮定については、後ほど詳しく説明します。

[]: ここでいう妥当であるとは、統計的に望ましい幾つかの性質を持つことを言います。これらの性質を持つとき、計算した推定値は真の値をある意味正しく推定できていると言えます。

## 回帰表の解釈
前節では、重回帰分析が、独立変数と従属変数の間の方程式の形の関係性における係数を推定する分析であることを説明しました。そこでこの節では、実際の学術論文で報告された回帰分析の結果の表を見ながら、どのように結果を解釈することができるのかを確認していきます。

### 回帰表と回帰係数


### 推定の不確実性と信頼区間、統計的仮説検定

### 回帰分析の指標、決定係数



## 回帰係数が正しく推定されるための仮定

これまで見てきたような回帰係数は、得られたデータから計算して推定されたものなのでした。では、この値は真の値を正しく推定できているのでしょうか。すでに述べた通り、推定した回帰係数が妥当であるためには、いくつかの仮定を満たしている必要があります。この節では、これらの仮定について具体的に説明します。そのためにまず先に4つの仮定を列挙し、その後それぞれについて説明します。計算式が列挙されていますが、それ自体を解釈する必要はありません (表現は西山ら(2019)に基づき、若干の変更を行った)。

1. i.i.d. の仮定
    - $(Y_i, X_{1i}, X_{2i},\cdots, X_{Mi}), i = 1, \cdots, n$ は、独立同一分布に従う。
2. 外生性の仮定
    - $E[\varepsilon_i\|X_{1i}, \cdots, X_{Mi}]=0$
3. 多重共線性がない
    - 任意の$\Sigma_{j}a_j^2 = 1$となる $a_0, \cdots, a_M$について、$E[(a_0 + a_1X_{1i} + \cdots + a_MX_{Mi}^2)] > 0$が成り立つ
4. 異常値がない
- $(X_{1i}, \cdots, X_{Mi}, \varepsilon_i)$ は有限の4次のモーメントを持つ

以下では数学的な議論というより、実際に分析を批判的に解釈する際に重要な考え方を述べます。先行研究の回帰分析が方法論的に正しく行われているのかを議論したり、自己の分析が妥当であることを主張したりする際の思考方法になります。


### 1. i.i.d. の仮定

<div class="summary-box">
  <p><strong>要約</strong></p>
  <ul>
    <li><b>i.i.d. (独立同一分布)</b> の仮定とは、各観測の変数の組が同一の分布に従って独立に実現しているという仮定</li>
    <ul>
      <li>この仮定が満たされていないと、回帰係数の標準偏差 (推定結果がどれくらい不確実か) が正しく推定されない</li>
    </ul>
    <li><b>独立</b>とはある観測の変数の実現値が別の観測の値の実現に影響を与えないこと</li>
    <ul>
      <li>主に<b>系列相関</b>と<b>クラスタ相関</b>により破れうる</li>
      <li>無作為抽出で対処可能。それが無理なら、自己回帰モデル・パネルデータ分析、固定効果モデルを用いる。</li>
    </ul>
    <li><b>同一分布</b>とは、変数が同じ分布から実現すること</li>
    <ul>
      <li><b>誤差項の分散不均一</b>が問題になることが多い</li>
      <li>不均一分散に頑健な標準誤差の計算方法を用いる</li>
    </ul>
  </ul>
</div>



i.i.d. (independent and identical distribution) の仮定とは、日本語で独立同一分布の仮定といい、各観測の変数の組が**同一の分布に従って独立に**実現することを指します。以下では、i.i.d. の独立の部分と同一分布の部分についてそれぞれ簡単に説明を行います。


まず**独立**とは、ある観測の変数の実現値が別の観測の値の実現に影響を与えないことをいいます[]。ある人の教育レベルは別の人の教育レベルに影響を与えそうですし、ある人が新聞をとっているかどうかは同棲している別の人の政治への関心に影響を与えそうです。独立性の仮定が満たされない代表的な例は、系列相関やクラスタ相関がある場合です。

[]: もちろん統計学的にはより厳密に定義されますが、ここでは省略します。

系列相関（または自己相関）とは、データに含まれる自己の過去の値が他の時期の値に影響を与えるようなことをいいます。例えば、あなたのある月の支出額は次の月のあなたの支出額と関係がありそうです。なぜなら、支出が多かった次の月は支出を減らそうと考えるかもしれないからであり、または、生活水準が上がり翌月もさらに豊かな暮らしをしようと支出を増やすかもしれないからです。

また、データのうち一部は同じ大学に通う人々かもしれず、同じコミュニティに属する人々は互いに影響を与えながら生活していることが予想されます。このような、特定の属性に固有の集団的な特徴が存在する場合、クラスタ相関が存在するといい、同様に独立性の仮定を破る原因になり得ます。

以上を踏まえれば、同じグループに属する対象や同じ対象そのものが複数回データに含まれる場合、i.i.d. が満たされない可能性があります。この問題に対処する最も原始的な方法は、**母集団に対して無作為抽出を実施する**ことです。しかし、社会科学において母集団に対する無作為抽出が行えることは稀であり (そもそも社会そのものが標本であるため)、構造的な理由により別の対処を考える必要があります。

自己相関の存在が疑われる場合、説明変数にラグを考慮した過去の値を表す変数を加えることにより対処するのが一般的です。これを一般化し、回帰モデルに時間の概念を加えたものが**パネルデータ分析**と呼ばれるものです。クラスター相関が疑われる場合、特定のグループに属しているかどうかのダミー変数をモデルに加えることにより対応する場合があります。この手法は固定効果 (Fixed Effect) モデルなどと言われたりします[]。

[]: これらのモデルを実際に分析で使う場合、標準誤差の計算方法に補正を加える必要があることに注意してください。パネルデータ分析やクラスタ頑健な標準誤差の計算方法については、計量経済学のテキストを参照することを強く勧めます。

次に同一分布とは、各変数が同じ分布から実現することをいいます。出やすさの異なるサイコロを振るのはだめで、同じサイコロを何度も振り直しているようなイメージです。この部分に関する代表的な問題は、不均一分散です。分散とは簡単に言えばばらつきのことですが、ここでは特に誤差項の分散に関する議論を扱っています。誤差項とは、理論的な推定値（予測値）と真の値（観測値）との差をいうのでした。その分散が不均一であるとは、ある独立変数が変化すると、従属変数のばらつきが大きくなったり小さくなったりすることを意味します。例えば、60代の人々の年収と20代の人々の年収を比べると、両者の間でばらつきが同じであるとはいえないでしょう。このような状態を分散不均一とか不均一分散といいます。社会科学においてはむしろ、分散が均一である状態のほうが稀であると言えます。

不均一分散は、もともとの分布に関する仮定が破れていることになるため、無作為抽出などによって根本的に解決することはできません。そこで方法論的には、標準誤差の計算方法を補正することで対処します。具体的には、不均一分散に頑健な標準誤差を計算することによって、分析の妥当性を主張する場合が多いです。

以下は若干発展的です。i.i.d. の仮定が満たされていない場合、回帰係数の点推定には問題が起こりませんが、回帰係数の標準偏差（ばらつき; 分散とほとんど同じ）の推定量である標準誤差が正しく計算されない問題が生じます。つまり、回帰係数の不確実性の範疇がわからなくなり、0である（つまり効果がない）かどうかの検定もできなくなってしまいます。回帰表に表示される項目で言えば、標準誤差、信頼区間、p値の解釈ができなくなってしまいます。これは、せっかく回帰係数を推定したのに、それがどれくらい正しそうであるかの判断を無効にするという意味で深刻な問題です。そのため、上記で見たような方法論的な対処が求められるということになります。


### 2. 外生性の仮定

<div class="summary-box">
  <p><strong>要約</strong></p>
  <ul>
    <li><b>外生性の仮定</b>とは、独立変数と誤差項が相関していないという仮定</li>
    <ul>
      <li>交絡因子をモデルに入れ忘れた際に生じる (<b>選択バイアス</b>)</li>
      <li>独立変数と従属変数に双方向の因果関係があるときに生じる (<b>同時性バイアス</b>)</li>
    </ul>
    <li>適切なモデルを構築することで対処が可能</li>
    <ul>
      <li>適切かどうかは理論的に説明するしかない</li>
    </ul>
    <li>理論的に適切なモデルを構築できない場合、別のモデルで対処する必要がある</li>
    <ul>
      <li><b>操作変数法</b>などの方法がある (この章では扱わない)</li>
      <li>上記のバイアスが生じていないかどうかを検定する方法もあるが、絶対ではない</li>
    </ul>
  </ul>
</div>

外生性 (exogeneity) の仮定とは、数学的には、独立変数を所与としたときに誤差項の期待値が $0$ であることをいいます。......と言われても正直ピンときません。

誤差項とは、理論値と実現値の差であると述べました。この差が存在することは、考慮している独立変数以外の変数によっても $Y$ の値が左右されていることを表しています。着目している独立変数 $\boldsymbol{X}$ がわかったときに誤差項が平均的に $0$ であるということは、$\boldsymbol{X}$ は (正確に $Y$ を予測できていないとしても) 平均的には正しく予測できていると言えます。すなわち、$\boldsymbol{X}$ を統制している条件下で、モデルは体系的な偏りを持っていないわけです。

では、$\boldsymbol{X}$ を統制している条件下で誤差項が体系的な偏りを持つとはどういうことでしょうか。繰り返しになりますが、誤差項は、$Y$ に影響を与えうる変数のうち、考慮している独立変数 $\boldsymbol{X}$ 以外の全てを含んでいます。これが独立変数により偏るということは、**誤差項に含まれる何らかの変数と独立変数との間で相関関係がある**ことを意味します。したがって、外生性の仮定が破れることは、そのような変数を適切にモデルに組み込めていないことと言い換えることができます。

以下では、外生性の仮定が満たされない状況を大きく2つ説明します。すなわち、(1) 選択バイアスが生じている場合と、(2) 同時性バイアスが生じている場合です。なお、外生性の仮定が満たされていない状況を、内生性の問題ということがあります。

第一に選択バイアス (セレクションバイアス) とは、独立変数にも従属変数にも影響を与える変数をモデルに組み込めていないときに生じる体系的な偏りのことをいいます。必要な外生変数が欠落しているという意味で欠落変数バイアス (脱落変数バイアス、OVB; Omitted Variable Bias) とも言います。また、このような変数を交絡因子と呼びます。

選択バイアスに対処する方法は、より適切なモデルを構築することにほかなりません。つまり、**独立変数以外に存在するあらゆる変数が、独立変数と相関を持たない**と主張できる回帰モデルを構築する必要があります。逆に言えば、交絡因子を一つ示すだけで推定結果に疑義を呈することができます。独立変数以外に存在するあらゆる変数が独立変数と相関を持たないこと、つまり交絡因子が存在しないことは、理論的に主張するほかありません (悪魔の証明というやつ)。そして、交絡因子と思われる変数は全てモデルに組み込んで回帰分析を行う必要があるということです。

内生性の問題を生じさせる第二の原因は、同時性バイアス[]です。これは、独立変数と従属変数の間で相互に因果関係が存在する場合に生じる問題です。有名な例は、需要と供給、犯罪率と警察署の数、2プレイヤーの意思決定などでしょうか[]。このように、従属変数から独立変数に向かう因果関係が存在する場合にも、外生性の仮定が満たされません。なお余談ですが、ある時期の従属変数が未来の独立変数に影響を与え、それを通して未来の従属変数にも影響を与えているという意味では、交絡因子と捉えて選択バイアスの一種であると考えることができるかもしれません。

[]: 政治学では「同時性バイアス」という名前はあまり一般的ではないかもしれません。双方向の因果関係が存在する等と言い換えることができます。この言葉は、ミクロ経済学における同時方程式の考え方 (モデル) に由来します。 

[]: 最後の例は若干理論的かもしれません。プレイヤーは相手の行動や応答すなわち従属変数の値を予測して、自己の行動 (独立変数) を決定する場合があります。このとき、従属変数から独立変数に対する因果関係が存在するといえます。

同時性バイアスについても、理論的な主張によってモデルの正しさを示すことしかできません。その意味で外生性の仮定は、分析を通して妥当性を主張する必要のある多少厄介な問題であるといえます。

なお、誤解されることが多いのですが、$Y$ に影響を与える変数をモデルに含めないことそれ自体は、分析に大きな影響を与えません。そのような変数が誤差項として捉えられていて、かつ、独立変数と相関を持っていないような場合、外生性の仮定は満たされています。したがって、着目する独立変数の回帰係数の推定には問題を生じさせません。もちろん、$Y$ を予測する精度は落ちると言えますが、私たちの目的は $Y$ の予測ではありません。平均的には誤差が $0$ であるのであれば、やはり分析上の問題とは言えないわけです。

最後に、(少なくとも政治学の分析において) 外生性の仮定を満たすためにモデルに加えた変数のことを特に統制変数といいます。式の見かけ上は独立変数と変わりありませんが、係数の値には (それが正しいのかさえ) 興味がなく、方法論上必要なためモデルに加えた変数のことです。そのような理由により加えているわけですから、**統制変数の回帰係数が正しく推定されるために努力する必要はありませ**んし、その意味で**統制変数の回帰係数は解釈してはいけません**。その値が信頼に値するかわかりませんし、そもそも信頼に値する必要がないのです。すべては着目する独立変数の回帰係数の推定が正しいかどうかです。


### 3. 多重共線性がない

<div class="summary-box">
  <p><strong>要約</strong></p>
  <ul>
    <li><b>多重共線性</b>とは、2つ以上の独立変数の間に強い相関関係があることをいう</li>
    <ul>
      <li><b>完全な多重共線性</b>：一方の値が決まるともう一方の値も決まる</li>
      <li><b>弱い多重共線性</b>：強い相関関係があり、高い精度でもう一方を予測できる</li>
    </ul>
    <li>完全な多重共線性が生じているとき、回帰係数は推定できない</li>
    <li>弱い多重共線性が生じているとき、回帰係数の推定が不安定になる</li>
    <ul>
      <li>サンプルサイズを大きくすることで対処する場合がある</li>
    </ul>
  </ul>
</div>

回帰係数が正しく推定される第3の仮定は、多重共線性がないことです。多重共線性とは、厳密には**独立変数の間に線形な (一次関数的な) 関係があること**と説明されます。噛み砕いて言えば、2つ以上の独立変数について、片方の値がわかるともう一方の値もわかる方程式的な関係にあるということです。このような厳密な意味での多重共線性を、「完全な多重共線性」ということがあります。例えば、ある商品のUSドル価格と日本円価格は、レートを掛け算するだけで交換可能なため、完全な多重共線性の関係にあります。

完全な多重共線性が生じているとき、モデルには2つ以上の**同じ情報を持った独立変数が含まれる**ことになります。この状態で回帰係数を推定しようとしても、各変数からの影響を区別することはできず、推定することができなくなります。これをパラメタの識別ができないと表現します。

完全な多重共線性が問題になることはめったにありません。なぜなら、モデルを注意深く見ていればそもそも起こりにくい問題ですし、万が一同じ変数をモデルに入れてしまっていた場合は、統計ソフトがそれを教えてくれるからです (推定できないよ！ とエラーが出る)。強いて言えば、完全な多重共線性の関係にあるが、一部のデータ入力のミスで完全でなくなってしまったような場合、エラーが出なくてわからないが推定が不安定という問題が生じることがあるかもしれません。この問題も、注意深くモデルを構築すれば起こりにくいことです。

多重共線性をより広い意味で捉えることがあります。すなわち、2つ以上の独立変数について、片方の値がわかるともう一方の値が高い精度で予測できるほどに強い相関関係がある場合です。これを弱い多重共線性といいます[]。例えば、期末テストの点数と中間テストの点数とかでしょうか。

[]: 強い相関関係があるのに弱い多重共線性というのは少し紛らわしく感じます。実際には、完全な多重共線性の仮定よりも弱い仮定という意味です。

弱い多重共線性が生じているとき、それらの変数の回帰係数の推定が不安定になることがあります。完全な多重共線性の問題と同様に、識別が困難なほどに強い関係性がある場合に生じる問題です。これに対処するには、弱い多重共線性が起きている変数のうち不要な方をモデルから除外するという方法が考えられます。その変数が特に理由もなくモデルに含まれているのであれば、この方法は有効です。しかし、両方が興味のある変数であれば、モデルから除外することはできません。また、外生性の仮定のために含んでいる場合も、除外すれば選択バイアスが生じます。また、統制変数同士に多重共線性が起きている場合であっても、その回帰係数が正しいかには興味がないため、やはりモデルから除外する必要はありません。

それを考えると、弱い多重共線性が存在しているからといって変数をモデルから除外する必要は必ずしもありません。回帰係数の推定の精度を上げるという意味では、サンプルサイズを大きくすることが有効な対処になります。

なお以上の説明から明らかなように、多重共線性の問題は2つ以上の独立変数がモデルに含まれる重回帰分析に固有の仮定です。これに対し、独立変数が1つしかないモデルを単回帰モデルといい、単回帰モデルでは多重共線性の問題はふつう問題になりません。


### 4. 異常値がない
最後の仮定は、モーメントという難しい単語が出ています。これは漸近正規性という統計的な性質を満たすために必要な仮定であって、理論的に解釈どうこうという仮定ではありません。ただしタイトルにもあるとおり、この仮定が満たされているとき、データは異常値 (明らかに大きいまたは小さい値であって、正しくないと思われるもの) がないことが保証されます。実際、データ分析を行う際には欠損値や異常値に適切に対処しないと結果に悪影響を与えることから、分析の際のデータの正しさを担保する仮定であると言えます。